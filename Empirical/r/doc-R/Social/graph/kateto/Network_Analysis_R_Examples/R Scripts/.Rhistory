ch.cug.cent <- cug.test(ch.net, centralization, cmode="edges", FUN.arg=list(FUN=degree, cmode="outdegree"))
ch.cug.cent
plot(ch.cug.cent)
ch.cug.tr <- cug.test(ch.net, gtrans, cmode="dyad") # Conditioning on dyad census
ch.cug.tr
plot(ch.cug.tr)
install.packages("ergm")
library(ergm)
data(florentine)
detach("package:ergm")
flobusiness; flomarriage
gcor(flomarriage,flobusiness)
flo.qap <- qaptest(list(flomarriage,flobusiness), gcor, g1=1, g2=2, reps=100)
flo.qap
plot(flo.qap)
x <- rgraph(10, 3)
x[1,,] # matrix 1
x[2,,] # matrix 2
x[3,,] # matrix 3
y <- 3*x[1,,] + 5*x[2,,] + 7
net.mod.1 <- netlm(y, x, reps=100)
summary(net.mod.1)
# Oh look - the first two parameters are significant and close to 3 and 5, and the intercept is 7
library(ergm)
library(sna)
data(florentine)
flobusiness; flomarriage;
help('ergm-terms')
flo.mar.1 <- ergm(flomarriage ~ edges)
flo.mar.1
summary(flo.mar.1)
log(20/(120-20)) # We get -1.609, the same as the edge parameter in the erg model.
# The corresponding probability is .167:
exp(-1.609)/(1+exp(-1.609)) # you can also get that using inv.logit() from package "boot"
flo.mar.2 <- ergm(flomarriage ~ edges + triangles, seed=1)
flo.mar.2
help('ergm-terms')
library(ergm)
library(sna)
data(florentine)
flobusiness; flomarriage;
help('ergm-terms')
flo.mar.1 <- ergm(flomarriage ~ edges)
flo.mar.1
summary(flo.mar.1)
log(20/(120-20)) # We get -1.609, the same as the edge parameter in the erg model.
# The corresponding probability is .167:
exp(-1.609)/(1+exp(-1.609)) # you can also get that using inv.logit() from package "boot"
flo.mar.2 <- ergm(flomarriage ~ edges + triangles, seed=1)
flo.mar.2 <- ergm(flomarriage ~ edges + triangles)
flo.mar.2
summary(flo.mar.2)
flo.mar.3 <- ergm(flobusiness ~ edges + edgecov(flomarriage))
flo.mar.3
summary(flo.mar.3)
w.vec <- flomarriage %v% 'wealth'  # Store the node wealth in a numeric vector.
w.vec
gplot(flomarriage, vertex.cex=w.vec/20)	# plot the network with vertex size proportional to wealth
flo.mar.4 <- ergm(flomarriage ~ edges + nodecov("wealth"))
flo.mar.4
summary(flo.mar.4)
data(samplk)
samplk1; samplk2; samplk3
plot(samplk3)
samp.mod.1 <- ergm(samplk3 ~ edges + mutual)
summary(samp.mod.1)
data(faux.mesa.high)
fmh.net <- faux.mesa.high
plot(fmh.net)
fmh.net
# Taking a look at gender
plot(fmh.net, vertex.col='Sex')
# Taking a look at the grade of the students
plot(fmh.net, vertex.col='Grade')
# Taking a look at the race of the students
plot(fmh.net, vertex.col='Race')
# A simple model that includes just the edge (density) parameter:
fmh.mod.1 <- ergm(fmh.net ~ edges)
summary(fmh.mod.1)
fmh.mod.2 <- ergm(fmh.net ~ edges + nodematch("Grade"))
summary(fmh.mod.2)
fmh.mod.3 <- ergm(fmh.net ~ edges + nodematch("Grade", diff=T))
summary(fmh.mod.3)
fmh.mod.4 <- ergm(fmh.net ~ edges + nodematch("Grade") + nodematch("Race") + nodematch("Sex"))
summary(fmh.mod.4)
fmh.mod.5 <- ergm(fmh.net ~ edges + nodemix("Race"))
summary(fmh.mod.5)
table(fmh.net %v% "Race")  			# Check out race frequencies
mixingmatrix(fmh.net, "Race")   # Check out # of links between/within groups
fmh.mod.6 <- ergm(fmh.net ~ edges + nodematch("Grade", diff = T) + nodefactor("Sex"))
summary(fmh.mod.6)
fmh.mod.7 <- ergm(fmh.net ~ edges + nodecov("Grade") + nodematch("Sex"))
summary(fmh.mod.7)
fmh.mod.8 <- ergm(fmh.net ~ edges + absdiff("Grade") + nodematch("Sex"))
summary(fmh.mod.8)
fmh.mod.8.sim <- simulate(fmh.mod.8, nsim=15)
summary(fmh.mod.8.sim)
# All the simulated network are stored in the returned object:
class(fmh.mod.8.sim)
# We can access any of them and take a look at it:
fmh.mod.8.sim[[1]]
summary(flo.mar.4) # Take a look at the model
flo.mar.4.gof <- gof(flo.mar.4 ~ degree) # goodness of fit for degree distribution
flo.mar.4.gof # Take a look at the observed & simulated values
plot(flo.mar.4.gof) # plot the observed & simulated values
flo.mar.4.gof2 <- gof(flo.mar.4 ~ distance, nsim=20) # gof based on 20 simulated nets
summary(flo.mar.4.gof2)
plot(flo.mar.4.gof2)
mcmc.diagnostics(flo.mar.2)
pdf("flo_mar_model2.pdf")
mcmc.diagnostics(flo.mar.2)
dev.off()
# We can examine the diagnostics to see whether our model looks ok,
# We can examine the diagnostics to see whether our model looks ok,
# check for model degeneracy, see if MCMC sample size and burn-in are large enough, etc.
# We can examine the diagnostics to see whether our model looks ok,
# check for model degeneracy, see if MCMC sample size and burn-in are large enough, etc.
# Read more about interpreting model diagnostics in Section 5 of this document:
#=============================================================#
#=============================================================#
flo.mar.1
summary(flo.mar.1)
flo.mar.1 <- ergm(flomarriage ~ edges)
flo.mar.1 <- ergm(flomarriage$edges)
library(ergm)
library(sna)
data(florentine)
flobusiness; flomarriage;
help('ergm-terms')
flo.mar.1 <- ergm(flomarriage ~ edges)
flo.mar.1
summary(flo.mar.1)
log(20/(120-20)) # We get -1.609, the same as the edge parameter in the erg model.
# The corresponding probability is .167:
exp(-1.609)/(1+exp(-1.609)) # you can also get that using inv.logit() from package "boot"
flo.mar.2 <- ergm(flomarriage ~ edges + triangles)
flo.mar.2
summary(flo.mar.2)
# The triangle coefficient is not significant - so this is not a signature
flo.mar.3 <- ergm(flobusiness ~ edges + edgecov(flomarriage))
flo.mar.3
summary(flo.mar.3)
w.vec <- flomarriage %v% 'wealth'  # Store the node wealth in a numeric vector.
w.vec
gplot(flomarriage, vertex.cex=w.vec/20)	# plot the network with vertex size proportional to wealth
flo.mar.4 <- ergm(flomarriage ~ edges + nodecov("wealth"))
flo.mar.4
summary(flo.mar.4)
data(samplk)
samplk1; samplk2; samplk3
plot(samplk3)
samp.mod.1 <- ergm(samplk3 ~ edges + mutual)
summary(samp.mod.1)
data(faux.mesa.high)
fmh.net <- faux.mesa.high
plot(fmh.net)
fmh.net
# Taking a look at gender
plot(fmh.net, vertex.col='Sex')
# Taking a look at the grade of the students
plot(fmh.net, vertex.col='Grade')
# Taking a look at the race of the students
plot(fmh.net, vertex.col='Race')
# A simple model that includes just the edge (density) parameter:
fmh.mod.1 <- ergm(fmh.net ~ edges)
summary(fmh.mod.1)
fmh.mod.2 <- ergm(fmh.net ~ edges + nodematch("Grade"))
summary(fmh.mod.2)
fmh.mod.3 <- ergm(fmh.net ~ edges + nodematch("Grade", diff=T))
summary(fmh.mod.3)
## Input data.
x = c( 370, 1016, 1235, 1419, 1567, 1820,
706, 1018, 1238, 1420, 1578, 1868,
716, 1020, 1252, 1420, 1594, 1881,
746, 1055, 1258, 1450, 1602, 1890,
785, 1085, 1262, 1452, 1604, 1893,
797, 1102, 1269, 1475, 1608, 1895,
844, 1102, 1270, 1478, 1630, 1910,
855, 1108, 1290, 1481, 1642, 1923,
858, 1115, 1293, 1485, 1674, 1940,
886, 1120, 1300, 1502, 1730, 1945,
886, 1134, 1310, 1505, 1750, 2023,
930, 1140, 1313, 1513, 1750, 2100,
960, 1199, 1315, 1522, 1763, 2130,
988, 1200, 1330, 1522, 1768, 2215,
990, 1200, 1355, 1530, 1781, 2268,
1000, 1203, 1390, 1540, 1782, 2440,
1010, 1222, 1416, 1560, 1792)
## Generate initial plots of the data.
par(mfrow=c(2,2),bg=rgb(1,1,0.8))
dotchart(x,xlab="Polished Window Strength (ksi)")
boxplot (x,ylab="Polished Window Strength (ksi)")
hist    (x,ylab="Counts",xlab="Polished Window Strength (ksi)",main="")
plot(density(x),xlab="Polished Window Strength (ksi)",main="")
## Generate QQ-plot of the data.
par(mfrow=c(1,1),bg=rgb(1,1,0.8))
qqnorm(x, pch=20, col="Red")
## Plot 99 samples from the data and compare QQ-plots to actual data.
x.ave = mean(x); x.sd = sd(x)
nx = length(x)
nb = 99
u = qnorm(ppoints(nx))
xb = array(dim=c(nx,nb))
for (jb in 1:nb) {
xb[,jb] = sort(qqnorm(rnorm(nx, mean=x.ave, sd=x.sd),
plot=FALSE)$y)}
par(mfrow=c(1,1),bg=rgb(1,1,0.8))
zz = qqnorm(x, pch=20, col="Red")
matplot(u, xb, type="p", pch=21, col="Blue", add=TRUE)
points(zz$x,zz$y, pch=20, col="Red")
## Compare QQ-plots of the data for various distributions
## using maximum likelihood estimation.
require(stats4)
negloglik.gau = function (mu, sigma) {
if (sigma < 0) {return(Inf)} else {
-sum(dnorm(x, mean=mu, sd=sigma, log=TRUE))}}
x.gau = mle(negloglik.gau, method="Nelder-Mead",
start=list(mu=mean(x), sigma=sd(x)))
x.gau@coef
negloglik.gam = function (alpha, lambda) {
if (any(c(alpha, lambda) < 0)) {return(Inf)} else {
-sum(dgamma(x, shape=alpha, rate=lambda, log=TRUE))}}
x.gam = mle(negloglik.gam, method="Nelder-Mead",
start=list(alpha=(mean(x)/sd(x))^2,
lambda=mean(x)/var(x)))
x.gam@coef
##>        alpha       lambda
##> 11.851717840  0.008459428
require("bs")
negloglik.bs = function (alpha, beta) {
if (any(c(alpha, beta) < 0)) {return(Inf)} else {
-sum(dbs(x, alpha, beta, log=TRUE))}}
x.bs = mle(negloglik.bs, method="Nelder-Mead",
start=list(alpha=0.25, beta=30))
x.bs@coef
negloglik.wei = function (xi, beta, eta) {
-sum(dweibull(x-xi, shape=beta, scale=eta, log=TRUE)) }
x.wei = mle(negloglik.wei, method="L-BFGS-B",
lower=c(-Inf, 0, 0),
start=list(xi=100, beta=2, eta=15))
x.wei@coef
## Plot overlaid density for each distribution.
xl = mean(x)-3*sd(x); xu=mean(x)+3*sd(x)
plot(density(x, from=xl, to=xu), ylim=c(0, 0.0012),main="")
curve(dnorm(x, mean=x.gau@coef[1], sd=x.gau@coef[2]),
from=xl, to=xu, col="LightBlue", add=TRUE, lwd=2)
curve(dgamma(x, shape=x.gam@coef[1], rate=x.gam@coef[2]),
from=xl, to=xu, col="Brown", add=TRUE, lwd=2)
curve(dbs(x, alpha=x.bs@coef[1], beta=x.bs@coef[2]),
from=xl, to=xu, col="Red", add=TRUE, lwd=2)
curve(dweibull(x-x.wei@coef[1], shape=x.wei@coef[2],
scale=x.wei@coef[3]),
from=xl, to=xu, col="Purple", add=TRUE, lwd=2)
legend(1700, 0.0012, bty="n",
legend=c("Data", "Gaussian", "Gamma",
"Birnbaum-Saunders", "Weibull"),
lty=c(1,1,1,1,1),
col=c("Black","LightBlue", "Brown", "Red", "Purple"))
## Generate QQ-plots for each distribution and overlay.
xy.gau = list(x=sort(qnorm(ppoints(x),
mean=x.gau@coef[1], sd=x.gau@coef[2])),
y=sort(x))
xy.gam = list(x=sort(qgamma(ppoints(x),
shape=x.gam@coef[1], rate=x.gam@coef[2])),
y=sort(x))
xy.bs = list(x=sort(qbs(ppoints(x),
alpha=x.bs@coef[1], beta=x.bs@coef[2])),
y=sort(x))
xy.wei = list(x=sort(x.wei@coef[1] + qweibull(ppoints(x),
shape=x.wei@coef[2], scale=x.wei@coef[3])),
y=sort(x))
plot(xy.gau, pch=15, col="LightBlue",
xlab="Theoretical Quantiles",
ylab="Sample Quantiles")
points(xy.gam, pch=16, col="Brown")
points(xy.bs, pch=17, col="Red")
points(xy.wei, pch=18, col="Purple")
legend(500, 2400, bty="n",
legend=c("Gaussian", "Gamma",
"Birnbaum-Saunders", "Weibull"), pch=c(15,16,17,18),
col=c("LightBlue", "Brown", "Red", "Purple"))
## Compute AIC and BIC for each distribution.
aic = c(GAU=AIC(x.gau, k=2), GAM=AIC(x.gam, k=2),
BS=AIC(x.bs, k=2), WEI=AIC(x.wei, k=2))
bic = c(GAU=AIC(x.gau, k=log(nx)), GAM=AIC(x.gam, k=log(nx)),
BS=AIC(x.bs, k=log(nx)), WEI=AIC(x.wei, k=log(nx)))
signif(cbind(AIC=aic, BIC=bic), 4)
post = exp(-0.5*(bic-1500))/sum(exp(-0.5*(bic-1500)))
cbind(signif(post, 2))
install.packages("partools")
##>        alpha       lambda
##> 11.851717840  0.008459428
require("bs")
negloglik.bs = function (alpha, beta) {
if (any(c(alpha, beta) < 0)) {return(Inf)} else {
-sum(dbs(x, alpha, beta, log=TRUE))}}
install.packages("bs")
## Compare QQ-plots of the data for various distributions
## using maximum likelihood estimation.
require(stats4)
##>        alpha       lambda
##> 11.851717840  0.008459428
require("bs")
negloglik.bs = function (alpha, beta) {
if (any(c(alpha, beta) < 0)) {return(Inf)} else {
-sum(dbs(x, alpha, beta, log=TRUE))}}
x.bs = mle(negloglik.bs, method="Nelder-Mead",
start=list(alpha=0.25, beta=30))
x.bs@coef
install.packages("splines2")
negloglik.bs = function (alpha, beta) {
if (any(c(alpha, beta) < 0)) {return(Inf)} else {
-sum(dbs(x, alpha, beta, log=TRUE))}}
x.bs = mle(negloglik.bs, method="Nelder-Mead",
start=list(alpha=0.25, beta=30))
install.packages("sna")
install.packages("network")
library("sna")
library("network")
library("igraph")
library("statnet")
install.packages("statnet")
library("statnet")
library("MASS")
detach("package:MASS")
data()        # list datasets
data(Cars93)  # load this built-on dataset (from package MASS)
Cars93        # take a look at the data
head(Cars93)  # look at the first 6 rows of the data
# Help on function called as.matrix.network
?as.matrix.network
# If you remember this function is in package "network":
?network::as.matrix.network
# Use ?? if you're not quite sure about the name of the function
??"network matrix"
# You can get help on built in datasets too
?Cars93  # A description of the dataset.
# Check the current working directory
getwd()
# Set the working directory
setwd("C:/USC/Classes/11-2012-Fall/COMM 645/_LABS_/R Folder")
ls()      # get a list of the objects you have created in the current R session.
search()  # get a list of all currently attached packages.
2 + 2
2^2+2*2
sqrt(abs(-4))
x <- 3         # Assignment
x              # R evaluates the expression and prints the result
y <- 4         # Assignment
y + 5          # Evaluation, note that y remains 4
z <- x + 17*y  # Assignment
z              # Evaluation
rm(z)          # Remove z, this deletes the object.
z              # Try to evaluate z... and fail.
2==2
2!=2
x <= y
5/0
is.inf(5/0)
0/0
is.nan(0/0)
v1 <- c(1, 7, 11, 22)       # Numeric vector, length 4
v2 <- c("hello","world")    # Character vector, length 2 (a vector of strings)
v3 <- c(TRUE, TRUE, FALSE)  # Logical vector, same as c(T, T, F)
(T+T)*(T+T+T)               # As you remember from Boolean algebra, T=1, F=0
v4 <- c(v1,v2,v3,"boo") 	#All elements magically turn into strings
v <- 1:7         # same as c(1,2,3,4,5,6,7)
v <- rep(0, 77)  # repeat zero 77 times: v is a vector of 77 zeroes
length(v)        # check the length of the vector
v <- rep(1:3, times=2) # Repeat 1,2,3 twice
v <- rep(1:10, each=2) # Repeat each element twice
v <- seq(10,20,2) # sequence: numbers between 10 and 20, in jumps of 2
v1 <- 1:5         # 1,2,3,4,5. This assignment erases the old value of v1.
v2 <- rep(1,5)    # 1,1,1,1,1.
v1 + v2      # Element-wise addition
v1 + 1       # Add 1 to each element
v1 * 2       # Multiply each element by 2
v1 + c(1,7)  # This doesn't work: (1,7) is a vector of different length
sum(v1)      # The sum of all elements
mean(v1)     # The average of all elements
sd(v1)       # The standard deviation
cor(v1,v1*5) # Correlation between v1 and v1*5
v1 > 2       # Each element is compared to 2, returns logical vector (TRUE or FALSE for each element)
v1==v2       # Are corresponding elements equivalent? Returns logical vector.
v1!=v2       # Are corresponding elements *not* equivalent? Same as !(v1==v2). Returns logical vector.
(v1>2) | (v2>0)  # | is the boolean OR, returns a vector.
(v1>2) & (v2>0)  # & is the boolean AND, returns a vector.
(v1>2) || (v2>0)  # || is the boolean OR - returns a single value
(v1>2) && (v2>0)  # && is the boolean AND - ditto
v1[3]             # third element of v1
v1[2:4]           # elements 2, 3, 4 of v1
v1[c(1,3)]        # elements 1 and 3 - note that your indexes are a vector
v1[c(T,T,F,F,F)]  # elements 1 and 2 - only the ones that are TRUE
v1[v1>3]          # v1>3 is a logical vector TRUE for elements >3
# Note that all comparisons return logical vectors that can be assigned to an object:
LogVec <- v1 > 3  # Logical vector, same length as v1, TRUE (or T) where v1 elements > 3
v1[LogVec]        # Returns only those elements from v1 that are > 3
# To add more elements to a vector, simply assign them values.
# Elements 5 to 10 are assigned values 5,6,7,8,9,10:
v1[6:10] <- 6:10
# We can also directly assign the vector a length:
length(v1) <- 15 # the last 5 elements are added as missing data: NA
m <- rep(1, 20)   # A vector of 20 elements, all 1
dim(m) <- c(5,4)  # Dimensions set to 5 & 4, so m is now a 5x4 matrix
m <- matrix(data=1, nrow=5, ncol=4)  # same matrix as above, 5x4, full of 1s
m <- matrix(1,5,4) 			         # same matrix as above
dim(m)                               # What are the dimensions of m?
m <- cbind(1:5, 5:1, 5:9)  # Bind 3 vectors as columns, 5x3 matrix
m <- rbind(1:5, 5:1, 5:9)  # Bind 3 vectors as rows, 3x5 matrix
m <- matrix(1:10,10,10)
m[2,3]  # Matrix m, row 2, column 3 - a single cell
m[2,]   # The whole second row of m as a vector
m[,2]   # The whole second column of m as a vector
m[1:2,4:6] # submatrix: rows 1 and 2, columns 4, 5 and 6
m[-1,]     # all rows *except* the first one
m[1,]==m[,1]  # Are elements in row 1 equivalent to corresponding elements from column 1?
m>3           # A logical matrix: TRUE for m elements >3, FALSE otherwise
m[m>3]        # Selects only TRUE elements - that is ones greater than 3
# Transpose m:
t(m)          # Again, note that this will not change the matrix,
# it only evaluates t(m) and prints the result
m <- t(m)     # Now m is transposed.
m %*% t(m)    # The operator %*% does matrix multiplication
m * m         # Note that * is element-wise multiplication
a <- array(data=1:18,dim=c(3,3,2)) # 3d with dimensions 3x3x2
a <- array(1:18,c(3,3,2))          # a shorter way to get the same array
eye.col.v <- c("brown", "green", "brown", "blue", "blue", "blue") #vector
eye.col.f <- factor(c("brown", "green", "brown", "blue", "blue", "blue")) #factor
levels(eye.col.f)  # The levels (distinct values) of the factor (categorical variable)
as.numeric(eye.col.f)  # The factor as numeric values: 1 is  blue, 2 is brown, 3 is green
as.numeric(eye.col.v)  # The character vector, however, can not be coerced to numeric
l1 <- list(boo=v1,foo=v2,moo=v3,zoo="Animals!")  # A list with four components
l2 <- list(v1,v2,v3,"Animals!")
l1["boo"]      # Access boo using square brackets - this returns a list.
l1[["boo"]]    # Access boo using double square brackets - this returns the numeric vector
l1[[1]]        # Returns the first component of the list, equivalent to above.
l1$boo         # Named elements can also be accessed using the $ operator - equivalent to [[]]
l1[[5]] <- "More elements!" # The list l1 had 4 elements, we're adding a 5th here.
l1[[8]] <- 1:11 # We added an 8th element, but not 6th or 7th. Those will be created empty (NULL)
l1$Something <- "A thing"  # Adds a ninth element - "A thing", called "Something"
head(Cars93)    # Let's take another look at the cars93 dataset
class(Cars93)   # What is the data format? Look, it's a data frame!
dfr1 <- data.frame(ID=1:4,FirstName=c("John","Jim","Jane","Jill"), Female=c(F,F,T,T), Age=c(22,33,44,55))
dfr1$FirstName   # This is the second column of dfr1. Notice that R thinks this is a categorical variable
# Let's get rid of the factor by telling R to treat FirstName as vector:
dfr1$FirstName <- as.vector(dfr1$FirstName)
# Alternatively, you can tell R you don't like factors from the start using stringsAsFactors=FALSE:
dfr2 <- data.frame(FirstName=c("John","Jim","Jane","Jill"),stringsAsFactors=FALSE)
dfr2$FirstName   # Success: not a factor.
library(sna)
library(network)
?emon
data(emon)
emon  # a list of 7 networks
# Cheyenne network (the first one in the list)
ch.net <- emon$Cheyenne
plot(ch.net)
ch.attr <- data.frame(matrix (0,14,8))
colnames(ch.attr) <-   c("vertex.names", "Command.Rank.Score", "Decision.Rank.Score",
"Formalization", "Location", "Paid.Staff", "Sponsorship",
"Volunteer.Staff")
# Copy each of the 8 vertex attributes to a variable in the ch.attr data frame
for (i in 1:8) { ch.attr[,i] <- (ch.net %v% colnames(ch.attr)[i]) }
ch.attr
cor(ch.attr[[2]], ch.attr[[3]])      # Calculate the correlation btw command and decision rank
cor.test(ch.attr[[2]], ch.attr[[3]]) # Examine the significance
ch.lm.1 <- lm(ch.attr[[2]] ~ ch.attr[[3]] + ch.attr[[6]] + ch.attr[[8]])
summary(ch.lm.1)
ch.attr$IndegCent  <- degree(ch.net, gmode="digraph", cmode="indegree")  # indegree centralities
ch.attr$OutdegCent <- degree(ch.net, gmode="digraph", cmode="outdegree") # outdegree centralities
ch.attr$BetweenCent <- betweenness(ch.net, gmode="digraph") # betweenness centralities
ch.lm.2 <- lm(ch.attr[[2]] ~ ch.attr[[6]] + ch.attr[[11]]) # Model with betweenness centrality
summary(ch.lm.2)
ch.cug.den <- cug.test(ch.net, gden, cmode="size")
ch.cug.den
ch.cug.recip <- cug.test(ch.net, grecip, cmode="edges")
ch.cug.recip
plot(ch.cug.recip)
ch.cug.cent <- cug.test(ch.net, centralization, cmode="edges", FUN.arg=list(FUN=degree, cmode="outdegree"))
ch.cug.cent
plot(ch.cug.cent)
ch.cug.tr <- cug.test(ch.net, gtrans, cmode="dyad") # Conditioning on dyad census
ch.cug.tr
plot(ch.cug.tr)
# install.packages("ergm")
library(ergm)
data(florentine)
detach("package:ergm")
flobusiness; flomarriage
gcor(flomarriage,flobusiness)
flo.qap <- qaptest(list(flomarriage,flobusiness), gcor, g1=1, g2=2, reps=100)
flo.qap
plot(flo.qap)
x <- rgraph(10, 3)
x[1,,] # matrix 1
x[2,,] # matrix 2
x[3,,] # matrix 3
y <- 3*x[1,,] + 5*x[2,,] + 7
net.mod.1 <- netlm(y, x, reps=100)
summary(net.mod.1)
# Oh look - the first two parameters are significant and close to 3 and 5, and the intercept is 7
#=============================================================#
w.vec <- flomarriage %v% 'wealth'  # Store the node wealth in a numeric vector.
w.vec
w.vec <- flomarriage %v% 'wealth'  # Store the node wealth in a numeric vector.
library(ergm)
library(sna)
w.vec <- flomarriage %v% 'wealth'  # Store the node wealth in a numeric vector.
attr1 <- as.matrix(read.table("data/s100-attr1.dat"))
attr2 <- as.matrix(read.table("data/s100-attr2.dat"))
attr3 <- as.matrix(read.table("data/s100-attr3.dat"))
source("script/setup.r")
